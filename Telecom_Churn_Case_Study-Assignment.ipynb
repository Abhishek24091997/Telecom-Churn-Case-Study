{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of the case studyÂ¶\n",
    "Creating a model which can easily find the customers with high probability of churn.\n",
    "Since prepaid is the kind of service that is used by most of indians, more focused about that.\n",
    "Finding out the customers with high probability of churn.\n",
    "Since 80% of the income earned by the top 20% of high value customers.\n",
    "Finding out the key variable of the customers who have high probability of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#importing the required things from sklearb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Warnings Suppressed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and installig the xgboost\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set\n",
    "telecom_churn_df = pd.read_csv(telecom_churn_data.csv\")\n",
    "telecom_churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find the description of the given data set\n",
    "telecom_churn_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting out the columns with high collinearity\n",
    "\n",
    "cor1 = telecom_churn_df.corr()\n",
    "for i in cor1.index:\n",
    "    for j in cor1.columns:\n",
    "        if ((i != j) and ((cor1.loc[i,j] > 0.65) or (cor1.loc[i,j] < -0.65))):\n",
    "            print('Correlation between',i,'&',j,'is  ',cor1.loc[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check for the duplicates in the data set\n",
    "sum(telecom_churn_df.duplicated(subset = 'mobile_number')) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the sum of all missing values\n",
    "Number_Of_Missing_Values = telecom_churn_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the sum of missing values percentage \n",
    "Percentage_Of_Missing_Values = 100 * telecom_churn_df.isnull().sum() / len(telecom_churn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating the two tables\n",
    "Dataframe_Missing_Value = pd.concat([Number_Of_Missing_Values, Percentage_Of_Missing_Values], axis=1)\n",
    "\n",
    "# Renameing the column names\n",
    "Columns_Missing_Values = Dataframe_Missing_Value.rename(columns={0: 'Number of Missing Values', 1: '% of Total Missing Values'})\n",
    "\n",
    "# Display the values in sorting order\n",
    "Columns_Missing_Values = Columns_Missing_Values[Columns_Missing_Values.iloc[:, 1] != 0].sort_values('% of Total Missing Values', ascending=False).round(2)\n",
    "print(\"Total number of missing columns are: \" + str(Columns_Missing_Values.shape[0]))\n",
    "print(Columns_Missing_Values.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unnecessary variables\n",
    "telecom_churn_df = telecom_churn_df.drop(['mobile_number','circle_id','last_date_of_month_6','date_of_last_rech_6','date_of_last_rech_data_6',\n",
    "                                          'last_date_of_month_7','date_of_last_rech_7','date_of_last_rech_data_7',\n",
    "                                          'last_date_of_month_8','date_of_last_rech_8','date_of_last_rech_data_8'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the value \"0\" to the nan values\n",
    "# telecom_churn_df.av_rech_amt_data_8.fillna(value=0, inplace=True)\n",
    "# telecom_churn_df.av_rech_amt_data_7.fillna(value=0, inplace=True)\n",
    "# telecom_churn_df.av_rech_amt_data_6.fillna(value=0, inplace=True)\n",
    "# telecom_churn_df.total_rech_data_8.fillna(value=0, inplace=True)\n",
    "# telecom_churn_df.total_rech_data_7.fillna(value=0, inplace=True)\n",
    "# telecom_churn_df.total_rech_data_6.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the Categorical variables by assigning them dummy's\n",
    "# Creating dummies for fb_user\n",
    "# dummy_fb_user_8 = pd.get_dummies(telecom_churn_df['fb_user_8'],drop_first=True)\n",
    "# dummy_fb_user_7 = pd.get_dummies(telecom_churn_df['fb_user_7'],drop_first=True)\n",
    "# dummy_fb_user_6 = pd.get_dummies(telecom_churn_df['fb_user_6'],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating the dummys with the original data set\n",
    "# telecom_churn_df = pd.concat([telecom_churn_df,dummy_fb_user_8],axis=1)\n",
    "# telecom_churn_df = pd.concat([telecom_churn_df,dummy_fb_user_7],axis=1)\n",
    "# telecom_churn_df = pd.concat([telecom_churn_df,dummy_fb_user_6],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies for night pack users\n",
    "# dummy_night_pck_user_8 = pd.get_dummies(telecom_churn_df['night_pck_user_8'],drop_first=True)\n",
    "# dummy_night_pck_user_7 = pd.get_dummies(telecom_churn_df['night_pck_user_7'],drop_first=True)\n",
    "# dummy_night_pck_user_6 = pd.get_dummies(telecom_churn_df['night_pck_user_6'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating the dummys with the original data set\n",
    "# telecom_churn_df = pd.concat([telecom_churn_df,dummy_night_pck_user_8],axis=1)\n",
    "# telecom_churn_df = pd.concat([telecom_churn_df,dummy_night_pck_user_7],axis=1)\n",
    "# telecom_churn_df = pd.concat([telecom_churn_df,dummy_night_pck_user_6],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the varibles for which we have created the dummies\n",
    "# telecom_churn_df = telecom_churn_df.drop(['fb_user_8', 'fb_user_7', 'fb_user_6', 'night_pck_user_8', 'night_pck_user_7', 'night_pck_user_6'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the shape of the data\n",
    "telecom_churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the basic stats about the dataset\n",
    "telecom_churn_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a variable named \"Churn\"\n",
    "telecom_churn_df['Churn'] = np.where( (telecom_churn_df['vol_2g_mb_9']==0 ) & (telecom_churn_df['total_ic_mou_9']==0) & (telecom_churn_df['total_og_mou_9']==0) & (telecom_churn_df['vol_3g_mb_9']==0), 1 ,0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the total rechange amount\n",
    "telecom_churn_df['Sum_of_Recharge_amount'] = telecom_churn_df['total_rech_amt_7'] + telecom_churn_df['total_rech_amt_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the average recharge amount\n",
    "telecom_churn_df['Average_Recharge_amount'] = telecom_churn_df['Sum_of_Recharge_amount']/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q70 = telecom_churn_df['Average_Recharge_amount'].quantile(0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.drop(telecom_churn_df[telecom_churn_df['Average_Recharge_amount'] <= Q70].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the columns with prefix \"_9\"\n",
    "drop_9_columns = [usg for usg in list(telecom_churn_df) if usg.endswith(\"_9\")]\n",
    "telecom_churn_df.drop(drop_9_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum of missing values\n",
    "Numberofmissingvalues = telecom_churn_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the X and y variables\n",
    "X=telecom_churn_df.drop('Churn', axis =1)\n",
    "y=telecom_churn_df.Churn\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plottingt the graph of ADR  vs Non- ADR\n",
    "f,ax = plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "colors = [\"Green\", \"Blue\"]\n",
    "labels = 'Non-Churned','Churned'\n",
    "plt.suptitle('Information on Churn vs NON-Churn labels')\n",
    "# Plotting the pie chart\n",
    "y.value_counts().plot.pie(explode=[0,0.05], autopct='%1.2f%%', ax =ax[0], shadow = True,\n",
    "                                    colors = colors, labels = labels, fontsize =12, startangle=25)\n",
    "ax[0].set_ylabel('% of ADR vs Non-ADR')\n",
    "\n",
    "palette = [\"Red\", \"Blue\"]\n",
    "# Plotting the Bar chart\n",
    "sns.countplot(x='Churn', data=telecom_churn_df)\n",
    "ax[1].set_xticklabels(['Non-Churn','Churn'],rotation=0, rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variable \"X_train\",\"X_test\",\"y_train\",\"y_test\"\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the shape of  X_train\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the shape of  X_test\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_C_A = PCA(svd_solver='randomized', random_state= 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_C_A.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(P_C_A.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Column_names = list(X_train.columns)\n",
    "P_C_A_dataframe = pd.DataFrame({'First_Component':P_C_A.components_[0],'Second_Component':P_C_A.components_[1], 'Selected_Feature':Column_names})\n",
    "P_C_A_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the principal components\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (10,20))\n",
    "plt.scatter(P_C_A_dataframe.First_Component, P_C_A_dataframe.Second_Component)\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "plt.title('Plot for principal components')\n",
    "plt.grid()\n",
    "\n",
    "for i, txt in enumerate(P_C_A_dataframe.Selected_Feature):\n",
    "    #Writinig down the annotation\n",
    "    plt.annotate(txt, (P_C_A_dataframe.First_Component[i],P_C_A_dataframe.Second_Component[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainaed variance analysis \n",
    "P_C_A.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.cumsum(P_C_A.explained_variance_ratio_),color='red')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "Final_Principal_Components = IncrementalPCA(n_components = 10)\n",
    "Prinicpal_component_dataframe = Final_Principal_Components.fit_transform(X)\n",
    "Prinicpal_component_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prinicpal_component_dataframe = pd.DataFrame(Prinicpal_component_dataframe)\n",
    "Prinicpal_component_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the correlation matrix for Principal component dataframe\n",
    "Correlation = np.corrcoef(Prinicpal_component_dataframe.transpose())\n",
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the correlation between the variables\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(Correlation,annot=True,linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train component and checking its shape\n",
    "train_Component = Final_Principal_Components.fit_transform(X_train)\n",
    "train_Component.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the test component and checking its shape\n",
    "test_Component = Final_Principal_Components.transform(X_test)\n",
    "test_Component.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let't make the predictions based on the model\n",
    "def get_score(model,X_train,X_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the logistic regression.\n",
    "Logistic_Classification = LogisticRegression(random_state=42, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(Logistic_Classification,X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(Logistic_Classification,train_Component,test_Component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the the random forest classification to cl;aasify the variables\n",
    "Random_Forest_Classification = RandomForestClassifier(random_state = 42, n_estimators= 100,n_jobs =-1, class_weight = {0:1,1:10})\n",
    "get_score(Random_Forest_Classification,X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { 'n_estimators':[150],'max_features': ['auto','log2','sqrt'] ,'class_weight':['balanced','balanced_subsample'],\n",
    "          'max_depth' : [2,4,8],'min_samples_leaf':[20,22,24],'criterion' : ['gini']}\n",
    "Grid_search = GridSearchCV(estimator = RandomForestClassifier(random_state = 42,n_jobs = -1 ), scoring = 'recall', cv =3,\n",
    "                                                           param_grid = parameters)\n",
    "Grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABC = Grid_search.best_params_\n",
    "print(ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter_test = {'min_child_weight':[5,6],'max_depth': range(3,5),'gamma':[0,0.2,0.4]}\n",
    "gridsearch = GridSearchCV(estimator = XGBClassifier(), param_grid = Parameter_test, scoring='recall',n_jobs=4,iid=False, cv=3)\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the xg boost for boosting the model\n",
    "XGModel = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=4, min_child_weight=7, gamma=0.4,nthread=4, \n",
    "                        subsample=0.8, colsample_bytree=0.8,objective= 'binary:logistic',scale_pos_weight=3,seed=29)\n",
    "XGModel.fit(X_train, y_train)\n",
    "y_xgboost = XGModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets find whether the model is overfitted or not\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the confusion matrix\n",
    "print(confusion_matrix(y_test, y_xgboost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict the value\n",
    "predictions = [value for value in y_xgboost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the accuracy of model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy_score: %.2f%% on test dataset\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the precision score of model\n",
    "precision = precision_score(y_test, predictions)\n",
    "print(\"precision_score: %.2f%% on test dataset\" % (precision * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# printing the recall score \n",
    "print(\"recall_score: %.2f%% on test dataset\" % (recall * 100.0))\n",
    "print(\"f1_score: %.2f%% on test dataset\" % (f1 * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the roc for training set\n",
    "print(\"roc_auc training set\", roc_auc_score(y_train, XGModel.predict_proba(X_train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the roc for test set\n",
    "print(\"roc_auc test set\", roc_auc_score(y_test, XGModel.predict_proba(X_test)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the graph based on, feature inmportance on the basis of gain.\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "plot_importance(XGModel,importance_type='gain', max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have use the threshold to select the model.\n",
    "selection = SelectFromModel(XGModel, threshold=0.06, prefit=True)\n",
    "select_X_train = selection.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "selection_model = XGBClassifier()\n",
    "selection_model.fit(select_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model\n",
    "select_X_test = selection.transform(X_test)\n",
    "y_pred = selection_model.predict(select_X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
